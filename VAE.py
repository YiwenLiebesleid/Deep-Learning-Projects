# -*- coding: utf-8 -*-
"""VAE.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DZ1ZD67tofwvaVx0xkFa4Ntr75kAZa2q
"""

from warnings import simplefilter
simplefilter(action='ignore', category=FutureWarning)
simplefilter(action='ignore', category=Warning)

import numpy as np
import pandas as pd
import os

import matplotlib.pyplot as plt
from keras.layers import Input, Dense, Lambda, Dropout
from keras.models import Model
from keras import backend as K
from keras.optimizers import Adam, Adagrad, RMSprop
from scipy.stats import norm

try:
    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
    print('Running on TPU ', tpu.master())
except ValueError:
    tpu = None

if tpu:
    tf.config.experimental_connect_to_cluster(tpu)
    tf.tpu.experimental.initialize_tpu_system(tpu)
    strategy = tf.distribute.experimental.TPUStrategy(tpu)
else:
    strategy = tf.distribute.get_strategy()

mnist = np.load('/content/drive/MyDrive/mnist_train_images.npy')

from google.colab import drive
drive.mount('/content/drive')

BATCHSIZE = 100
epochs = 130

def EncoderModel(inputs):
  x = Dense(256, activation='relu')(inputs)
  # x = Dropout(0.3)(x)
  x = Dense(64, activation='relu')(x)

  mu = Dense(2)(x)
  sigma = Dense(2)(x)
  
  epsilon = K.random_normal(shape = tf.shape(mu), mean = 0.0, stddev = 1, dtype = tf.float32)
  z = mu + epsilon * K.exp(sigma)
  
  encoder = Model(inputs, [mu, sigma, z])
  return encoder, mu, sigma

def DecoderModel():
  z = Input(shape = (2,))
  x = Dense(64, activation='relu')(z)
  x = Dense(256, activation='relu')(x)
  outputs = Dense(784, activation='sigmoid')(x)

  decoder = Model(z, outputs)
  return decoder

def VAEModel():
  inputs = Input(shape = (784,))
  encoder, mu, sigma = EncoderModel(inputs)
  decoder = DecoderModel()

  outputs = decoder(encoder(inputs)[2])
  VAE = Model(inputs, outputs)

  # VAE loss
  reconstruct = K.sum(K.binary_crossentropy(inputs, outputs), axis = 1)
  KL = -0.5 * K.sum(1 + sigma - K.square(mu) - K.exp(sigma), axis = -1) * 5
  Vloss = reconstruct + KL

  VAE.add_loss(Vloss)
  VAE.compile(optimizer=RMSprop(lr=0.0008))
  # VAE.compile(optimizer=Adam(lr=0.0005))
  # VAE.summary()
  return VAE, decoder

def train(VAE, X_train, epochnums):
  # VAE.fit(X_train, batch_size=BATCHSIZE, epochs=epochs, verbose=1)
  
  batchnum = len(X_train) // BATCHSIZE
  for epoch in range(epochnums):
    for batchid in range(batchnum):
      batch = X_train[batchid * BATCHSIZE: (batchid + 1) * BATCHSIZE]
      total = np.mean(VAE.train_on_batch(batch))
    print("epoch {}, loss: {}".format(epoch + 1, total))

if __name__ == "__main__":

  VAE, decoder = VAEModel()
  train(VAE, mnist, epochs)
  # train(VAE, mnist, 10)

  # plot result
  n = 40
  rows = 1
  digit_size = 28
  figure = np.zeros((digit_size * rows * 3, digit_size * n))
  for r in range(rows):
    for i in range(n):
    figure[r*digit_size:(r+1)*digit_size,i*digit_size:(i+1)*digit_size] = mnist[i+r*100:i+1+r*100].reshape(28, 28)
    figure[(r+rows)*digit_size:(r+rows+1)*digit_size,i*digit_size:(i+1)*digit_size] = VAE.predict(mnist[i+r*100:i+1+r*100]).reshape(28, 28)
  
  for r in range(rows):
    for i in range(n):
    z_sample = np.array([np.random.normal(0, 1, 2)])
    x_decoded = decoder.predict(z_sample)
    digit = x_decoded[0].reshape(digit_size, digit_size)
    figure[(r+rows+rows)*digit_size:(r+rows+rows+1)*digit_size,i*digit_size:(i+1)*digit_size] = digit

  plt.figure(figsize=(25, 25))
  plt.imshow(figure, cmap='Greys_r')
  plt.show()
